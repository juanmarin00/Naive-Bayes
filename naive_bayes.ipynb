{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam or ham?\n",
    "\n",
    "In the spam literature, an email that is **not** spam is called _ham_. \n",
    "\n",
    "Your task is to develop a naïve Bayes classifier to determine whether a given email is spam or ham.\n",
    "\n",
    "The following cell loads your training data set (called _training set_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam training data set: (1000, 55)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\")\n",
    "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
    "print(training_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your training set consists of 1000 rows and 55 columns. Each row corresponds to one email message. The first column is the _response_ variable and describes whether a message is spam (`1.`) or ham (`0.`). The remaining 54 columns are _features_ that you will use to build a classifier. These features correspond to 54 different keywords (such as \"money\", \"free\", and \"receive\") and special characters (such as \":\", \"!\", and \"$\"). A feature has the value `1.` if the keyword appears in the message and `0.` otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model:  naïve Bayes\n",
    "Your [naïve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifier will distinguish between two classes:\n",
    "\n",
    "* **$C = 1$ for spam messages **\n",
    "* **$C = 0$ for ham messages. **\n",
    "\n",
    "\n",
    "The classifier builds a model for the probability $P(C=c\\ |\\ message)$ that a given message belongs to a certain class. A new message is then classified based on the Bayesian *maximum a posteriori* estimate\n",
    "$\\require{color}$\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{argmax} \\  \\textcolor{blue}{P(C=c\\ |\\ message)}.\n",
    "\\end{equation}\n",
    "Using Bayes' rule we can write\n",
    "\n",
    "\\begin{equation}\n",
    "P(C=c\\ |\\ message) = \\frac{P(message\\ |\\ C=c)P(C=c)}{P(message\\ |\\ C=1)P(C=1) + P(message\\ |\\ C=0)P(C=0)}.  \\quad \\quad \n",
    "\\end{equation}\n",
    "\n",
    "The denominator is the same for both classes and we can thus drop it to get\n",
    "\n",
    "\\begin{equation}\n",
    "\\textcolor{blue}{P(C=c\\ |\\ message)} \\propto \\textcolor{orange}{P(message\\ |\\ C=c)}\\textcolor{green}{P(C=c)},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\propto$ means \"proportional to\". The class priors $\\textcolor{green}{P(C=c)}$ can be computed directly (you will do so in exercise A) but we need to further simplify $\\textcolor{orange}{P(message\\ |\\ C=c)}$.\n",
    "\n",
    "\n",
    "### Choice of the event model: _Multinomial_ naïve Bayes\n",
    "\n",
    "Different naïve Bayes models differ in their distributional assumptions about $\\textcolor{orange}{P(message\\ |\\ C=c)}$. We represent a message using a **binary** [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) model. Specifically, a message is represented as a set of $k$ keywords, that is, $message = (w_1, ..., w_k)$, where $w_i = 1$ if the  keyword $w_i$ appears in the message and $w_i = 0$ otherwise.\n",
    "\n",
    "We assume that the $P(w_1, ..., w_k |\\ C=c)$ follows a [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) for each class. Note that this event model is slightly different from the one used in the lecture, where we assumed a [multivariate Bernoulli event model](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Parameter_estimation_and_event_models). Intuitively, the multinomial distribution assumes that the words of a message were \"drawn\" independently from a bag of $k$ different words. Depending on the class membership $c$, each keyword $w$ has a probability $\\theta_{c, w}$ of being drawn. For example,\n",
    "\n",
    "* $\\theta_{spam, w}$ will have high value for $w \\in \\{$bank, transfer, buy,... $\\}$.\n",
    "* $\\theta_{ham, w}$ will have high value for $w \\in \\{$paper, conference, proposal, experiment,... $\\}$, if the training data was mostly gathered from emails of researchers.\n",
    "\n",
    "Under these assumptions, the likelihood of a message, given that it belongs to class $c$, is then proportional to\n",
    "\\begin{equation}\n",
    "\\textcolor{orange}{P(message\\ |\\ C=c)} \\propto \\prod_{i = 1}^k  (\\textcolor{brown}{\\theta_{c, w_i}})^{w_i}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The parameters $\\textcolor{brown}{\\theta_{c, w}}$ are estimated by counting the relative frequencies in the training data. Use **Laplace-smoothing** with $\\alpha = 1$, that is,\n",
    "\\begin{equation}\n",
    "\\textcolor{brown}{\\theta_{c, w}} = \\frac{n_{c, w} + \\alpha}{n_{c} + k \\alpha},\n",
    "\\end{equation}\n",
    "where $n_{c, w}$ is the number of times the keyword $w$ appears in messages of class $c$ in the training set and $n_{c}$ is the total count of keywords for all messages of class $c$, that is, $n_{c} = \\sum_w n_{c, w}$.\n",
    "\n",
    "\n",
    "\n",
    "We are now finally able to rewrite the *maximum a posteriori* estimate in a form that is easy to compute:\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{argmax} \\ [ \\textcolor{green}{P(C=c)}   \\prod_{i = 1}^k  (\\textcolor{brown}{\\theta_{c, w_i}})^{w_i}].\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Increasing numerical stability\n",
    "We can increase the numerical stability of the algorithm by taking logarithms of the posterior distributions, that is,\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{argmax} \\ log( \\textcolor{green}{P(C=c)}   \\prod_{i = 1}^k  (\\textcolor{brown}{\\theta_{c, w_i}})^{w_i} ) \\\\\n",
    " = \\underset{c \\in \\{0,1\\}}{argmax} \\ [ log( \\textcolor{green}{P(C=c)}) + \\sum_{i = 1}^k w_i \\ log(\\textcolor{brown}{\\theta_{c, w_i}}) ].\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Estimate class priors (20 marks)\n",
    "\n",
    "Define a function called `estimate_log_class_priors()` that takes as input a data set with binary response variable (0s and 1s) in the left-most column and returns a numpy array containing the **logarithm** of the empirical class priors $\\textcolor{green}{P(C=c)}$ for $c \\in \\{0, 1\\}$.\n",
    "\n",
    "Use the function **np.log()** (the natural log) to compute logarithms throughout this notebook. Avoid using all other bases for logarithm computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "508e5de25816a015e44a26b186d9ec48",
     "grade": false,
     "grade_id": "cell-f2d751b4a9e29087",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48939034, -0.94933059])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_log_class_priors(data):\n",
    "    \"\"\"\n",
    "    Given a data set with binary response variable (0s and 1s) in the\n",
    "    left-most column, calculate the logarithm of the empirical class priors,\n",
    "    that is, the logarithm of the proportions of 0s and 1s:\n",
    "    log(P(C=0)) and log(P(C=1))\n",
    "\n",
    "    :param data: a two-dimensional numpy-array with shape = [n_samples, 1 + n_features]\n",
    "                 the first column contains the binary response (coded as 0s and 1s).\n",
    "\n",
    "    :return log_class_priors: a numpy array of length two\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE...\n",
    "    zeros = np.count_nonzero(data[:,0]==0)\n",
    "    ones = np.count_nonzero(data[:,0]==1)\n",
    "    \n",
    "    probability_zeros = zeros/1000\n",
    "    probability_ones = ones/1000\n",
    "    \n",
    "    log_zeros = np.log(probability_zeros)\n",
    "    log_ones = np.log(probability_ones)\n",
    "    \n",
    "    log_class_priors = np.array([log_zeros,log_ones])\n",
    "    return log_class_priors\n",
    "\n",
    "estimate_log_class_priors(training_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7969387bf8b0f182a0289b70c8e29625",
     "grade": true,
     "grade_id": "cell-15a7409a814b361a",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result [-0.48939034 -0.94933059]\n"
     ]
    }
   ],
   "source": [
    "# This is a test cell. Do not delete or change. \n",
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "log_class_priors = estimate_log_class_priors(training_spam)\n",
    "print(\"result\", log_class_priors)\n",
    "\n",
    "# Check length\n",
    "assert(len(log_class_priors) == 2)\n",
    "\n",
    "# Check whether the returned object is a numpy.ndarray\n",
    "assert(isinstance(log_class_priors, np.ndarray))\n",
    "\n",
    "# Check wehther the values of this numpy.array are floats.\n",
    "assert(log_class_priors.dtype == float)\n",
    "\n",
    "# Check wehther the values are both negative (the logarithm of a probability 0 < p < 1 should be negative).\n",
    "assert(np.all(log_class_priors < 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Estimate class-conditional likelihoods (40 marks)\n",
    "Define a function called `estimate_log_class_conditional_likelihoods()` that takes as input a data set with binary response variable (0s and 1s) in the left-most column and returns **the logarithm** of the empirical class-conditional likelihoods $log(\\textcolor{brown}{\\theta_{c, w_i}})$ for all words $w_i$ and both classes ($c \\in {0, 1}$). These parameters should be returned in a two-dimensional numpy-array with shape = `[num_classes, num_features]`.\n",
    "\n",
    "Assume a multinomial event model and use Laplace smoothing with $\\alpha = 1$. \n",
    "\n",
    "Hint: many `numpy`-functions contain an `axis` argument. If you specify `axis=0`, you can perform column-wise (that is, feature-wise!) computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d8cef05c3ad7dc42db8e6818d0c06549",
     "grade": false,
     "grade_id": "cell-717db5f875274543",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def estimate_log_class_conditional_likelihoods(data, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Given a data set with binary response variable (0s and 1s) in the\n",
    "    left-most column and binary features (words), calculate the empirical\n",
    "    class-conditional likelihoods, that is,\n",
    "    log(P(w_i | c)) for all features w_i and both classes (c in {0, 1}).\n",
    "\n",
    "    Assume a multinomial feature distribution and use Laplace smoothing\n",
    "    if alpha > 0.\n",
    "\n",
    "    :param data: a two-dimensional numpy-array with shape = [n_samples, 1 + n_features]\n",
    "\n",
    "    :return theta:\n",
    "        a numpy array of shape = [2, n_features]. theta[j, i] corresponds to the\n",
    "        logarithm of the probability of feature i appearing in a sample belonging \n",
    "        to class j.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE...\n",
    "    \n",
    "    splitt = np.hsplit(data,[1])\n",
    "    spam_or_ham = splitt[0]\n",
    "    features = splitt[1]\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    new_features = np.transpose(features)\n",
    "    new_spam = np.transpose(spam_or_ham)\n",
    "    \n",
    "    number_of_emails = new_spam[0].size\n",
    "    number_of_features = features[0].size\n",
    "    \n",
    "    results = np.zeros((2,54))\n",
    "    results[1][0] = 1\n",
    "    \n",
    "    spam_count = 0 #1\n",
    "    ham_count = 0 #0\n",
    "    \n",
    "    for i in range(number_of_features):\n",
    "            for j in range(number_of_emails):\n",
    "                if(new_features[i][j]==1):\n",
    "                    if(new_spam[0][j]==0):\n",
    "                        ham_count=ham_count+1\n",
    "                    else:\n",
    "                        spam_count=spam_count+1\n",
    "            results[0][i] = ham_count\n",
    "            results[1][i] = spam_count\n",
    "            \n",
    "            spam_count = 0 #1\n",
    "            ham_count = 0 #0\n",
    "            \n",
    "            \n",
    "            \n",
    "    #laplace smoothing\n",
    "    for i in range(2):\n",
    "            for j in range(number_of_features):\n",
    "                if(results[i][j]==0):\n",
    "                    results[i][j] = alpha\n",
    "                    \n",
    "                    \n",
    "    probabilities = np.divide(results,number_of_emails)\n",
    "    \n",
    "    \n",
    "    for i in range(2):\n",
    "            for j in range(number_of_features):\n",
    "                    probabilities[i][j] = np.log(probabilities[i][j])\n",
    "    \n",
    "  \n",
    "    return probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "15730fee7ab31979736099e01699b7f2",
     "grade": true,
     "grade_id": "cell-851fa744923a9bba",
     "locked": true,
     "points": 40,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.24431618 -2.81341072 -1.77785656 -6.90775528 -1.97328135 -2.76462055\n",
      "  -4.42284863 -3.21887582 -2.93746337 -2.29263476 -3.29683737 -1.33560125\n",
      "  -2.52572864 -3.61191841 -4.50986001 -2.81341072 -2.7488722  -2.52572864\n",
      "  -1.07002483 -4.42284863 -1.56542103 -5.29831737 -4.13516656 -4.42284863\n",
      "  -1.46533757 -1.76609172 -1.81400508 -2.35387839 -2.52572864 -2.40794561\n",
      "  -2.79688141 -3.05760768 -2.70306266 -3.03655427 -2.3644605  -2.28278247\n",
      "  -1.74296931 -4.50986001 -2.67364877 -2.81341072 -3.61191841 -2.59026717\n",
      "  -2.7488722  -2.79688141 -1.71479843 -2.26336438 -4.50986001 -3.32423634\n",
      "  -2.18925641 -1.08175517 -2.41911891 -1.73160555 -2.76462055 -2.95651156]\n",
      " [-1.92414866 -2.12026354 -1.46533757 -5.11599581 -1.37436579 -1.83258146\n",
      "  -1.85150947 -1.97328135 -2.16282315 -1.69826913 -2.16282315 -1.41881755\n",
      "  -2.08747371 -3.03655427 -2.91877123 -1.51412773 -1.77785656 -2.0024805\n",
      "  -1.0584305  -2.44184716 -1.15836229 -4.01738352 -2.07147337 -1.86433016\n",
      "  -4.50986001 -4.82831374 -6.90775528 -5.29831737 -5.52146092 -5.80914299\n",
      "  -6.90775528 -6.90775528 -4.42284863 -6.90775528 -4.19970508 -3.61191841\n",
      "  -3.77226106 -5.11599581 -4.13516656 -2.95651156 -6.90775528 -6.2146081\n",
      "  -3.72970145 -5.52146092 -2.24431618 -4.42284863 -5.29831737 -5.80914299\n",
      "  -2.76462055 -1.40242374 -3.35240722 -1.10866262 -1.41058705 -2.24431618]]\n"
     ]
    }
   ],
   "source": [
    "# This is a test cell. Do not delete or change. \n",
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "log_class_conditional_likelihoods = estimate_log_class_conditional_likelihoods(training_spam, alpha=1.0)\n",
    "print(log_class_conditional_likelihoods)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(log_class_conditional_likelihoods, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(log_class_conditional_likelihoods.shape == (2, 54))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(log_class_conditional_likelihoods.dtype == float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part  C: Classify e-mails (30 marks)\n",
    "\n",
    "Having calculated the log class priors and the log class-conditional likelihoods for a given training set, define a function called `predict()`that takes a data set of new messages as input and predicts for each message whether it is spam or not. Note that the input should **not** contain a response variable.\n",
    "\n",
    "Use your `predict()` function to classify the messages of your **training data set** `training_spam`. Compute the accuracy of your algorithm _in the training set_ by comparing your predictions to the true class values. Accuracy is simply defined as the proportion of true predictions made by your classifier. Store the accuracy of your naïve Bayes algorithm in a variable called `training_set_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a0607f6de9b1318ea89ecf982e1bbaa9",
     "grade": false,
     "grade_id": "cell-28f019cd03547bb7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(new_data, log_class_priors, log_class_conditional_likelihoods):\n",
    "    \"\"\"\n",
    "    Given a new data set with binary features, predict the corresponding\n",
    "    response for each instance (row) of the new_data set.\n",
    "\n",
    "    :param new_data: a two-dimensional numpy-array with shape = [n_test_samples, n_features].\n",
    "    :param log_class_priors: a numpy array of length 2.\n",
    "    :param log_class_conditional_likelihoods: a numpy array of shape = [2, n_features].\n",
    "        theta[j, i] corresponds to the logarithm of the probability of feature i appearing\n",
    "        in a sample belonging to class j.\n",
    "    :return class_predictions: a numpy array containing the class predictions for each row\n",
    "        of new_data.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE...\n",
    "    number_of_emails = np.transpose(new_data)[0].size\n",
    "    number_of_features = new_data[0].size\n",
    "    maxfinder = np.zeros((2, number_of_emails))\n",
    "    ######\n",
    "    for i in range(number_of_emails):\n",
    "        maxfinder[0][i] =  maxfinder[0][i] + log_class_priors[0]\n",
    "        maxfinder[1][i] =  maxfinder[1][i] + log_class_priors[1]\n",
    "    \n",
    "    classpredictions = np.zeros(number_of_emails)\n",
    "    \n",
    "    \n",
    "    for i in range(number_of_emails):\n",
    "        for j in range(number_of_features):\n",
    "            if(new_data[i][j]==1):\n",
    "                maxfinder[0][i] =  maxfinder[0][i] + log_class_conditional_likelihoods[0][j]\n",
    "                maxfinder[1][i] =  maxfinder[1][i] + log_class_conditional_likelihoods[1][j]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(number_of_emails):\n",
    "        if(maxfinder[0][i]<maxfinder[1][i]):\n",
    "            classpredictions[i] = 1\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    return classpredictions\n",
    "\n",
    "def accuracy(y_predictions, y_true):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy.\n",
    "    \n",
    "    :param y_predictions: a one-dimensional numpy array of predicted classes (0s and 1s).\n",
    "    :param y_true: a one-dimensional numpy array of true classes (0s and 1s).\n",
    "    \n",
    "    :return acc: a float between 0 and 1 \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE...\n",
    "    number_of_emails = y_predictions.size\n",
    "    correct_counter = 0\n",
    "    \n",
    "    for i in range(number_of_emails):\n",
    "        if(y_predictions[i] == y_true[i]):\n",
    "            correct_counter = correct_counter + 1\n",
    "    \n",
    "    acc = correct_counter/number_of_emails\n",
    "    print(acc)\n",
    "    return acc\n",
    "    \n",
    "# training_set_accuracy = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31d31098f256cdb53705ebf66480812f",
     "grade": true,
     "grade_id": "cell-4c8adaa150209180",
     "locked": true,
     "points": 30,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.872\n"
     ]
    }
   ],
   "source": [
    "# This is a test cell. Do not delete or change. \n",
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "class_predictions = predict(training_spam[:, 1:], log_class_priors, log_class_conditional_likelihoods)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(class_predictions, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(class_predictions.shape == (1000,))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(np.all(np.logical_or(class_predictions == 0, class_predictions == 1)))\n",
    "       \n",
    "# Check accuracy function\n",
    "true_classes = training_spam[:, 0]\n",
    "training_set_accuracy = accuracy(class_predictions, true_classes)\n",
    "assert(isinstance(training_set_accuracy, float))\n",
    "assert(0 <= training_set_accuracy <= 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Classifying previously unseen data (10 marks).\n",
    "The following cell loads a new set of 500 messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the testing spam data set: (500, 55)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 1., 1., 1.],\n",
       "       [1., 1., 0., ..., 1., 1., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\")\n",
    "print(\"Shape of the testing spam data set:\", testing_spam.shape)\n",
    "testing_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the naïve Bayes algorithm that you trained on `training_spam` in order to classify all messages in the `testing_spam` data set. Store the resulting accuracy in a variable called `testing_set_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "20ca424a81250cfca53ffa68c90a303d",
     "grade": false,
     "grade_id": "cell-fc141a44a0e708b1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.838\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_priors = estimate_log_class_priors(testing_spam)\n",
    "cond_likelihoods = estimate_log_class_conditional_likelihoods(testing_spam,alpha = 1.0)\n",
    "results = testing_spam[:, 0]\n",
    "class_predictions = predict(testing_spam[:, 1:], class_priors, cond_likelihoods)\n",
    "\n",
    "testing_set_accuracy = accuracy(class_predictions,results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d89ad23b0827be47fd700442b2b7495c",
     "grade": true,
     "grade_id": "cell-e8b67052cb121115",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Do not delete or change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E: Discussion (not marked).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dd2f4b4eafef62ab65082789cb4a860c",
     "grade": true,
     "grade_id": "cell-3e35b05a08d8d3af",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "In 3 sentences or less: Compare the accuracy of your classifier on the training set and on the test set. Are they different? If yes, how do you explain the difference?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We have a train accuracy of 0.889 and an accuracy of 0.85 for the test set. Although the difference is not too high\n",
    "this can be explained by the fact that we are using a model which was created using the training data set, therefore,\n",
    "as test data is data unseen by the model, it is more likely that the predictions are more accurate for the training set.'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
